---
layout: post
title:  Oaktable World Day 1
---
# Hash Joins and Bloom Filters
Toon Koppelaars @toonkoppelaars

Bloom filters try to speed up large hash joins in DW environments (parallel and serial plans)

The mother of all hash joins

```sql
select *
from emp e,dept d
where d.deptno = e.deptno
```

see exec plan in slides

applies hash function to each row and puts values in "buckets" (in pga).
Same hash function is applied to all rows on other table.
Search buckets where matching deptno hashes *should* exist and join.

## Filter early

```sql
select *
from emp e,dept d
where d.deptno = e.deptno
  and d.deptno between 130 and 150
```

injects and additional filter that e.deptno will be between 130 and 150.  fewer rows are sent to be hash joined as a result.

## Bloom Filters
Burton Howard Bloom 1970

- memory structure representing a set of values (say S)
- answers: Is a given value x, member of set S?
- two answers: definately not, or possibly yes.

- Bit array initially filled with 0's
- apply hash function to values to populate 1's at different offsets in array *see slides*
- test for membership if given value by applying hash-function to given value
- if all values at given offsets are 1, then value is possibly yes.

[bloom filter demo](https://www.jasondavies.com/bloomfilter/)
[bloom filter calculator](http://hur.st/bloomfilter)

Bloom filters help filter early in hash joins

bloom filters are used when there's a hash join with an additional filter on the smaller table.


```sql
select *
from emp e,dept d
where d.deptno = e.deptno
  and d.loc='UTRECHT'
```

uses bloom filter on e.deptno ```SYS_OP_BLOOM_FILTER```

```JOIN FILTER CREATE```
```JOIN FILTER USE```

## Bloom pruning filter (11g)
does partition pruning.
```PART JOIN FILTER CREATE```

event 10128 bloom filter pruning


# Dephix User Experience
Bobby Durrett
US Foods

## Before Delphix
- import/export for refresh
- refresh took several days
- only performed partial refresh
- DEV/QAT used expensive SAN, had less memory
- subsets of production data

## With Delphix
Benefits:
- refresh from production takes minutes
- delphix UI - point and click

Challenges:
- network performance issues
- slow disks
- performance difference due to direct I/O - required by delphix, bypass buffer cache

Delphix uses direct NFS...

## RPI Tweeting Bear
Kellyn
Oracle Education Foundation

Components:
- rasberry pi
- pi camera

code runs in background at atartup via the rc.local

STE[A]M - science, technology, education, art, math..?

- 41 states do not require any technical education
- only 10% of american high schools offer CS courses
**Push issue with state representitives**

Invest Locally:
- Devox4Kids
- Local User groups
- Local IT Groups
- After school programs
- Employment volunteerism


Start at age 6.

excess s - python class

teach using Nano.


# Using Database In-Memory Column Store with Complex Datatypes
Marco Gralike
xmldb.nl

XMLTYPE (TABLE or REF)
if you pick the wrong data type, you'll have a decrease in performance or get incorrect results

# NoSQL
Jonah H. Harris @jonahharris
meetme.com

mongodb
- schema-less
- highly available
- it's fast
- it crashes

www.mongodb-is-web-scale.com

mongrel.io
- mongo + relational
- maps documents to tables
- schema is implicit
- speaks mongodb wire-level protocol

its on github

# OpenZFS cross-platform migrations
Tim Gorman

- two societies separated by the same language

OpenZFS adaptive endiannes
Delphix XPP or U2L


# Virtual Columns
Jonathan Lewis

Why?
- bad cardinality
- functions on Columns

virtual columns provide better stats
VCs may be completely invisible to the application (especially in 12c)
- invisible columns don't get caught in a select *

12c: can add invisible to column

trunc(date) <= date

virtual column functions have to be executed for each row.  can slow down stats if not careful.


# Volkswagen for Oracle Guys
Martin Kier
Gmbh

volkswagen started by making cars for nazi's
oracle started by developing databases for cia for spying on russians.

NOx emissions <-> dbms_sql_translator

IT = Power, Use it wisely


# Performance
Cary Millsap @carymillsap

[free book](method-r.com)

Surrogate measures suck.
 - utilizations, latencies, samples, hit ratios

The code path for measuring software belongs in your tools, not your brain.

Some requirements
- relevance - measured durations must match experienced durations
- determinism - data interpretation proces must be deterministic, unambiguous, devoid of magic
- integrity - numbers must reconcile within a report, across drill-downs, across reports, etc.
- quality - no unit-mixing, no chartjunk, no longjumps, etc.

```mrskew```

## The Magic of Interposing

sql trace event 10046 data
$ORACLE_HOME/rdbms/mesg/oraus.msg - list trace event numbers, oracle sudo error debugging events


in trace file, there is a ```c=#### ```
is it user running? kernel running? both?  BOTH!

1 centisecond = 10000 microseconds

c = utime+stime

oracle makes two write calls for every trace written.
combine write into one operation using interposing.  can do buffered tracing this way.


# Internet of Things
Alex Gorbachev
Pythian

## IOT General Architecture

sensors -> field network hub -> global network

common protocols:
- http/https - ineffecient
- mqtt - lightweight

information sent to real time processing and/or analytics store

## Azure IoT stack - Event Hub

## AWS IOT stack

aws equivalents:
kafka == kinesis
spark == lambda
hdsf == cassandra

## Oracle's IOT cloud?
Exa-everything
It actually works well

google: cisco iot protocol


# Adaptive Dynamic Sampling
Chris Antognini @chrisantognini

## Introduction

Part of adaptive query optimization.
generating additional information during the parse phase of a query

## When is it used?
Enabled when OPTIMIZER_DYNAMIC_SAMPLING is set to 11.
OPTIMIZER_FEATURES_ENABLE >= 12.1.0.1

It is a 12c feature that was back ported to 11.2.0.4.

with sql plan directives
- sql plan directives instruct the database engine to either:
  1. use dynamic sampling
  2. create extended statistics

Optimizer can use ADS for all sql statements

## Time Limit
Optimizer bases the sampling on time limit
minimum is 1 sec.  
If SQL is in cursor cach or AWR:
- CONTROL_MANAGEMENT_PACK_ACCESS is evaluated by 12.1.0.2 only

If sql *not* present, the time limit defaults to **10 seconds**
Might be good to set that default lower.  Every new sql could take 10 secs to parse

If present, depends on cpu utilization and number of executions.

Can search database for sqlids to see how much ADS is being used for statements in cursor cache/AWR.

## Strategies
Block sampling
Sampling percentage changes based on size of table.  

Can choose to use full table scan or index scan.
each table in where caluse at least one sampling query is excuted.

for every index, when it's relevant, you may see several sampling queries.

Query Block Cardinality Adjustments (12.1.0.2)
try to get information about an aggregation.

## Caching

1. internal cache - use single parse operation for many sampling queries
2. result cache - many parse operations (EE feature)

## Fallacies
BUG# 19728268: Partition-Extended Names Not Applied

v$sql_plan.other_xml can contain wrong dynamic sampling level

## Summary
ADS can provide useful information to the optimizer
More parse time can be required
Needs to be carefully tested
Every version has a slightly different implementation


# Big Data

## Oracle Big Data Preparation

Most time spent on preparing data.  





















## Accidental Siri Activation Count: 3
