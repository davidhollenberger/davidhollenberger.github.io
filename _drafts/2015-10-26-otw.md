---
layout: post
title:  OOW - Monday Oaktable World
---
# Opening Session

Change title from DBA to Platform Scientist

# Minimizing the Concurrency Footprint of Tranactions
Mark W. Farnham

Concurrency footprint - duration of possibly blocking something else

cloud == time sharing

small transactions
[method r](www.method-r.com)

Instrument Your Code - control code on database using PL/SQL, not from client.  especially on cloud.
- transactions can get stranded if there's a network issue
- send in code from client, then commit
- Use stored procedure to avoid shipping code from client to db
- pre-load big things at "slack time" when there's extra resources

Hermitize popular rows
- to help scale when item becomes very popular
- partition so rows are contained in its own blocking
- add dummy items to get a new block, add real one, then pad rest of block with dummy data.

# SQL Translation Framework
Kerry Osbourne @KerryOracleGuy

- New in 12c r1
- translate sql statements on the fly
- some problems can only be fixed by changing the sql
- baselines, profiles are hint based
- change any select to another select statement

1. Create Translation Profile - sql mappings (register_sql_translations)
2. Tell session to use the Profile
3. Set the *10601* event

[sql translation framework](http://kerryosborne.oracle-guy.com/2013/07/sql-translation-framework/)

```sql
desc dba_sql_translations;
@translations --kerry's script
@create_translation --kerry's script
```

Materialized View Rewrite
*dbms_advanced_rewrite* - powerful, flakey, restrictions

```!oerr ora 900```

# Indexes Col groups
Jonathan Lewis

if you change an index, you will mess things up
get information from a column group that we used to get from an index.

12c auto-pilot - adaptive plan if there is a lot more data
- run query again and get different plan

gather_table_stats() - no more adaptive plan, it gets it right.

- allowed 20 col groups per table. *increases time to gather stats.*
- Oracle automatically creates col groups.  can be different between prod/test.

# The Cloud
John King

nist.gov/itl/cloud...
- on-demand self service
- broad network access
- resource pooling
- rapid elasticity
- measured service

What's your organization's security budget?  What do you think Oracle's/Amazon's budget for security?  Your data is more secure in the cloud...

# Graphing Outliers
Eric Grancher

Outliers matter
- perception
- averages lie

Graphing
- R
- fio
- SLOB - releasing slob addendum soon
- v$active_session_history


# AWR Ambiguity: Performance reasoning when the numbers don't add up
John Beresniewicz (JB)

Top 10 sql adding up to more than 100

DB Time = DB CPU time + Wait time

ASH and DB Time
DB Time ~ COUNT(sampled session) * interval
every sample is either on cpu or waiting

Trust DB Time and DB CPU time the most
Always get ASH report with AWR report.
Be wary of ASH CPU and measured wait times

# Modern Platform Topics for Modern DBAs
Kevin Closson

## Xtremeio
- even writes and wear to ssd
- global dedup
- virtual copies - read write "snapshots"

## Platform Topics

Slob for studying contention
- produces max iops per db cpu as possible.
- use "hot schema" method to generate significant concurrency class waits (v2.3+)
- *hot_schema_frequency* increase from modulo 2 to 97 to never

Busy waits are probably much more important than you think.

don't equate the percentage of these waits with the amount of performance degredation.  Use slob to research these.

Parallel Query
all your cpu is going to joins and sorts.

we're all too paranoid about what costs what.

modern systems are capable of huge volumes of data flow.

Swingbench
Everything is a CPU problem.
investigate swingbench with cpu affinity
effect on I/O wait times
use ```numactl --physcpubind```
consider CGROUPS


# Connecting Oracle with Hadoop
Tanel Poder

Hadoop is ready for the "enterprise"

gluent (glue enterprise data together)

Clouds are here to stay
Private datacenters aren't going anywhere.  "plumbing" needed to connect them.

Hadoop is here to stay
traditional DBMS aren't going anywhere. also needs "plumbing".

- hadoop is not a silo.  it's one data, many engines. (hive, impala)
- a copy of all enterprise data can be in hadoop.
- use for analytics and non-transactional processing
- push processing to hadoop as well

Scalability vs Features
- sql engine not as sophisticated as Oracle.

Scenarios
1. Load data from hadoop to oracle
2. Offload data from oracle to hadoop.
3. Query hadoop data in oracle.

sqoop
- data copy tool
- connect to oracle with jdbc.

```sqoop import --connect <jdbc_thin_connection> ...```

use cases
1. copy oracle to hadoop
2. copy hadoop to oracle
3. copy changed rows since last run

read oracle using full table scans and direct path reads.
load data into oracle via direct path insert or merge (bypass buffer cache and most undo/redo generation).

Download vm and run sqoop.  play with hdfs.

Update in Hive actually runs insert to a delta table.  It knows how to query data from deltas.
Other engines keep delta table in memory, and can be persisted in flash.

Other vendors:
Quest Shareplex (dell)
continuent Tungsten (vmware)

**Ultimate Goal:** Query hadoop data as if it was in (Oracle) RDBMS.

Can query hadoop over db link.  
You get some basic filter push down when querying data on hadoop






























## Accidental Siri Activation Count: 2
